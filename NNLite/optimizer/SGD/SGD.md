# Stochastic gradient descent (SGD)
In SGD, the parameter updates depend linearly on the current gradient and the learning rate, with no dependence on past gradients. This makes SGD perhaps the simplest optimizer to implement.

$$\theta_{new} = \theta - \eta\nabla_\theta E(\theta)$$






